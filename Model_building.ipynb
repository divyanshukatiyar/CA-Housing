{"cells":[{"cell_type":"code","metadata":{"tags":[],"cell_id":"00000-c336ffea-27cf-40a4-87d8-a5204443cd36"},"source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np","execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00001-b8f8ee10-1df4-485b-8a60-7f3c6ed460ca"},"source":"df = pd.read_csv('housing_cleaned.csv')\ndf.columns","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"Index(['Unnamed: 0', 'longitude', 'latitude', 'housing_median_age',\n       'total_rooms', 'total_bedrooms', 'population', 'households',\n       'median_income', 'median_house_value', 'ocean_proximity'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00002-c36fae4e-6af6-4499-b17d-b1edd78ac3f1"},"source":"# We don't want latitude and longitude\ndf_model = df[['housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income',\n 'median_house_value', 'ocean_proximity']]","execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00003-00c556be-1fa7-479a-9f07-c8bbe64c343b"},"source":"dummy = pd.get_dummies(df_model)","execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00004-2de50f02-65ba-477b-9bc9-c5ce63618a09"},"source":"# We have to split our data into training data and test data\nfrom sklearn.model_selection import train_test_split\n\nX = dummy.drop('median_house_value', axis=1)\ny = dummy.median_house_value.values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=2, random_state=1)","execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00005-163ebb3a-686f-4c5e-9d21-2d6d051f48e9"},"source":"!pip install statsmodels","execution_count":8,"outputs":[{"name":"stdout","text":"Collecting statsmodels\n  Downloading statsmodels-0.12.0-cp37-cp37m-manylinux1_x86_64.whl (9.5 MB)\n\u001b[K     |████████████████████████████████| 9.5 MB 3.4 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: pandas>=0.21 in /opt/venv/lib/python3.7/site-packages (from statsmodels) (1.0.5)\nRequirement already satisfied: scipy>=1.1 in /opt/venv/lib/python3.7/site-packages (from statsmodels) (1.5.1)\nRequirement already satisfied: numpy>=1.15 in /opt/venv/lib/python3.7/site-packages (from statsmodels) (1.19.0)\nCollecting patsy>=0.5\n  Downloading patsy-0.5.1-py2.py3-none-any.whl (231 kB)\n\u001b[K     |████████████████████████████████| 231 kB 36.5 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /opt/venv/lib/python3.7/site-packages (from pandas>=0.21->statsmodels) (2020.1)\nRequirement already satisfied: python-dateutil>=2.6.1 in /opt/venv/lib/python3.7/site-packages (from pandas>=0.21->statsmodels) (2.8.1)\nRequirement already satisfied: six in /opt/venv/lib/python3.7/site-packages (from patsy>=0.5->statsmodels) (1.15.0)\nInstalling collected packages: patsy, statsmodels\nSuccessfully installed patsy-0.5.1 statsmodels-0.12.0\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\nYou should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00005-00a27822-707a-4ce1-a8c2-7253a53c3298"},"source":"import statsmodels.api as sm\n\nX_sm = X = sm.add_constant(X)\nmodel = sm.OLS(y, X_sm)\nmodel.fit().summary()","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.634\nModel:                            OLS   Adj. R-squared:                  0.634\nMethod:                 Least Squares   F-statistic:                     3570.\nDate:                Sun, 06 Sep 2020   Prob (F-statistic):               0.00\nTime:                        17:22:06   Log-Likelihood:            -2.5950e+05\nNo. Observations:               20640   AIC:                         5.190e+05\nDf Residuals:                   20629   BIC:                         5.191e+05\nDf Model:                          10                                         \nCovariance Type:            nonrobust                                         \n==============================================================================================\n                                 coef    std err          t      P>|t|      [0.025      0.975]\n----------------------------------------------------------------------------------------------\nconst                        4.44e+04   5555.690      7.993      0.000    3.35e+04    5.53e+04\nhousing_median_age          1170.4825     44.193     26.486      0.000    1083.861    1257.104\ntotal_rooms                   -6.2773      0.779     -8.056      0.000      -7.805      -4.750\ntotal_bedrooms                57.4326      5.985      9.597      0.000      45.702      69.163\npopulation                   -38.1254      1.075    -35.459      0.000     -40.233     -36.018\nhouseholds                    99.7270      6.698     14.889      0.000      86.598     112.856\nmedian_income               4.003e+04    333.738    119.942      0.000    3.94e+04    4.07e+04\nocean_proximity_<1H OCEAN  -1.643e+04   5279.552     -3.113      0.002   -2.68e+04   -6084.549\nocean_proximity_INLAND      -8.46e+04   5271.505    -16.048      0.000   -9.49e+04   -7.43e+04\nocean_proximity_ISLAND      1.602e+05    2.6e+04      6.149      0.000    1.09e+05    2.11e+05\nocean_proximity_NEAR BAY   -1.205e+04   5398.562     -2.233      0.026   -2.26e+04   -1471.090\nocean_proximity_NEAR OCEAN -2683.8934   5351.425     -0.502      0.616   -1.32e+04    7805.322\n==============================================================================\nOmnibus:                     4990.754   Durbin-Watson:                   0.965\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            17791.441\nSkew:                           1.191   Prob(JB):                         0.00\nKurtosis:                       6.875   Cond. No.                     1.71e+19\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 1.1e-27. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\n\"\"\"","text/html":"<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th>  <td>   0.634</td>  \n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.634</td>  \n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   3570.</td>  \n</tr>\n<tr>\n  <th>Date:</th>             <td>Sun, 06 Sep 2020</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n</tr>\n<tr>\n  <th>Time:</th>                 <td>17:22:06</td>     <th>  Log-Likelihood:    </th> <td>-2.5950e+05</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td> 20640</td>      <th>  AIC:               </th>  <td>5.190e+05</td> \n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td> 20629</td>      <th>  BIC:               </th>  <td>5.191e+05</td> \n</tr>\n<tr>\n  <th>Df Model:</th>              <td>    10</td>      <th>                     </th>      <td> </td>     \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n               <td></td>                 <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th>                      <td>  4.44e+04</td> <td> 5555.690</td> <td>    7.993</td> <td> 0.000</td> <td> 3.35e+04</td> <td> 5.53e+04</td>\n</tr>\n<tr>\n  <th>housing_median_age</th>         <td> 1170.4825</td> <td>   44.193</td> <td>   26.486</td> <td> 0.000</td> <td> 1083.861</td> <td> 1257.104</td>\n</tr>\n<tr>\n  <th>total_rooms</th>                <td>   -6.2773</td> <td>    0.779</td> <td>   -8.056</td> <td> 0.000</td> <td>   -7.805</td> <td>   -4.750</td>\n</tr>\n<tr>\n  <th>total_bedrooms</th>             <td>   57.4326</td> <td>    5.985</td> <td>    9.597</td> <td> 0.000</td> <td>   45.702</td> <td>   69.163</td>\n</tr>\n<tr>\n  <th>population</th>                 <td>  -38.1254</td> <td>    1.075</td> <td>  -35.459</td> <td> 0.000</td> <td>  -40.233</td> <td>  -36.018</td>\n</tr>\n<tr>\n  <th>households</th>                 <td>   99.7270</td> <td>    6.698</td> <td>   14.889</td> <td> 0.000</td> <td>   86.598</td> <td>  112.856</td>\n</tr>\n<tr>\n  <th>median_income</th>              <td> 4.003e+04</td> <td>  333.738</td> <td>  119.942</td> <td> 0.000</td> <td> 3.94e+04</td> <td> 4.07e+04</td>\n</tr>\n<tr>\n  <th>ocean_proximity_<1H OCEAN</th>  <td>-1.643e+04</td> <td> 5279.552</td> <td>   -3.113</td> <td> 0.002</td> <td>-2.68e+04</td> <td>-6084.549</td>\n</tr>\n<tr>\n  <th>ocean_proximity_INLAND</th>     <td> -8.46e+04</td> <td> 5271.505</td> <td>  -16.048</td> <td> 0.000</td> <td>-9.49e+04</td> <td>-7.43e+04</td>\n</tr>\n<tr>\n  <th>ocean_proximity_ISLAND</th>     <td> 1.602e+05</td> <td>  2.6e+04</td> <td>    6.149</td> <td> 0.000</td> <td> 1.09e+05</td> <td> 2.11e+05</td>\n</tr>\n<tr>\n  <th>ocean_proximity_NEAR BAY</th>   <td>-1.205e+04</td> <td> 5398.562</td> <td>   -2.233</td> <td> 0.026</td> <td>-2.26e+04</td> <td>-1471.090</td>\n</tr>\n<tr>\n  <th>ocean_proximity_NEAR OCEAN</th> <td>-2683.8934</td> <td> 5351.425</td> <td>   -0.502</td> <td> 0.616</td> <td>-1.32e+04</td> <td> 7805.322</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>4990.754</td> <th>  Durbin-Watson:     </th> <td>   0.965</td> \n</tr>\n<tr>\n  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>17791.441</td>\n</tr>\n<tr>\n  <th>Skew:</th>           <td> 1.191</td>  <th>  Prob(JB):          </th> <td>    0.00</td> \n</tr>\n<tr>\n  <th>Kurtosis:</th>       <td> 6.875</td>  <th>  Cond. No.          </th> <td>1.71e+19</td> \n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 1.1e-27. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00006-94f42077-0b93-4e50-9373-a072b260131d"},"source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\nlinreg = LinearRegression()\nlinreg.fit(X_train,y_train)\n\nnp.mean(cross_val_score(linreg, X_train, y_train, scoring = 'neg_mean_absolute_error', cv=3))","execution_count":20,"outputs":[{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"-50608.21113768435"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00008-01c29b15-61ff-4207-a37d-d5d20bf8794f"},"source":"from sklearn.linear_model import Lasso\nlass = Lasso(alpha=1.0)\nlass.fit(X_train, y_train)\nnp.mean(cross_val_score(lass, X_train, y_train, scoring = 'neg_mean_absolute_error', cv=3))","execution_count":21,"outputs":[{"name":"stderr","text":"/opt/venv/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8212152970449.891, tolerance: 27479149682.445934\n  positive)\n/opt/venv/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5514935400095.109, tolerance: 18473020580.204212\n  positive)\n/opt/venv/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3182332775195.9297, tolerance: 18080082754.239674\n  positive)\n/opt/venv/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5523368145337.6875, tolerance: 18403883356.261425\n  positive)\n","output_type":"stream"},{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"-50609.514676399966"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00009-a32991e5-b2cc-4607-b0cd-b7d9da39e553"},"source":"from sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor()\nnp.mean(cross_val_score(rf, X_train, y_train, scoring = 'neg_mean_absolute_error', cv=3))","execution_count":33,"outputs":[{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"-43575.25937317583"},"metadata":{}}]},{"cell_type":"markdown","source":"#### We see that the mean error has reduced considerably using Random forest","metadata":{"tags":[],"cell_id":"00010-05116b12-0af3-4478-8098-5f0e1c4c43a4"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00012-33ed0aa9-3b72-4570-a773-7b225a1e0675"},"source":"# predictions\npred_linreg = linreg.predict(X_test)\npred_lass = lass.predict(X_test)\nrf.fit(X_train, y_train)\npred_rf = rf.predict(X_test)","execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00012-06d09a97-0553-4397-b1eb-f264c8763f1c"},"source":"from sklearn.metrics import mean_absolute_error\nprint(mean_absolute_error(y_test, pred_linreg))\nprint(mean_absolute_error(y_test, pred_lass))\nprint(mean_absolute_error(y_test, pred_rf))","execution_count":37,"outputs":[{"name":"stdout","text":"67323.631805632\n67324.43688826\n16902.45000000001\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Random forest works the best for this dataset. Its mean absolute error is almost 6 times lower than the other two models, so it gives the best predictions.","metadata":{"tags":[],"cell_id":"00013-085b1557-583c-4bb1-b31e-53edeed51628"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00014-54d6256c-eda4-4be3-81c1-81eae57f55dd"},"source":"# Best prediction\nprint(\"Best predicted house price is: $\" + str(pred_rf[0]))","execution_count":39,"outputs":[{"name":"stdout","text":"Best predicted house price is: $322269.1\n","output_type":"stream"}]}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"0ae88f59-0c7a-4a89-a6b7-aa551d23eed6","deepnote_execution_queue":[]}}